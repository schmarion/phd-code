{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    retry_if_exception_type,\n",
    "    stop_after_attempt,\n",
    "    stop_after_delay,\n",
    ")\n",
    "import openai\n",
    "import spacy\n",
    "\n",
    "from olaf import Pipeline\n",
    "from olaf.commons.errors import MissingEnvironmentVariable\n",
    "from olaf.commons.kr_to_rdf_tools import (\n",
    "    kr_concepts_to_owl_classes, kr_relations_to_owl_obj_props, \n",
    "    kr_metarelations_to_owl, kr_relations_to_anonymous_some_parent, concept_lrs_to_owl_individuals\n",
    ")\n",
    "from olaf.commons.logging_config import logger\n",
    "from olaf.commons.llm_tools import LLMGenerator\n",
    "from olaf.pipeline.pipeline_component.axiom_extraction import OWLAxiomExtraction\n",
    "from olaf.pipeline.pipeline_component.concept_relation_extraction import (\n",
    "    AgglomerativeClusteringConceptExtraction, \n",
    "    AgglomerativeClusteringRelationExtraction\n",
    ")\n",
    "from olaf.pipeline.pipeline_component.concept_relation_hierarchy import LLMBasedHierarchisation\n",
    "from olaf.pipeline.pipeline_component.term_extraction import LLMTermExtraction\n",
    "from olaf.repository.serialiser import KRJSONSerialiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"\"\n",
    "KR_PATH = \"\"\n",
    "ONTO_OWL_PATH = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KG building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH, 'r') as f:\n",
    "    file_content = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\" \".join(extract.lower().split()) for extract in file_content.split('#') if len(extract)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_model = spacy.load(\"fr_core_news_lg\")\n",
    "spacy_corpus = list(spacy_model.pipe(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT4oMiniGenerator(LLMGenerator):\n",
    "    \"\"\"Text generator based on OpenAI gpt-4o-mini model.\"\"\"\n",
    "\n",
    "    def check_resources(self) -> None:\n",
    "        \"\"\"Check that the resources needed to use the OpenAI Generator are available.\"\"\"\n",
    "        if \"OPENAI_API_KEY\" not in os.environ:\n",
    "            raise MissingEnvironmentVariable(self.__class__, \"OPENAI_API_KEY\")\n",
    "\n",
    "    def generate_text(self, prompt: list[dict[str, str]]) -> str:\n",
    "        \"\"\"Generate text based on a chat completion prompt for the OpenAI gtp-4o-mini model.\"\"\"\n",
    "\n",
    "        @retry(\n",
    "            stop=stop_after_delay(15) | stop_after_attempt(3),\n",
    "            retry=(\n",
    "                retry_if_exception_type(\n",
    "                    openai.APIConnectionError\n",
    "                    | openai.APITimeoutError\n",
    "                    | openai.RateLimitError\n",
    "                    | openai.InternalServerError\n",
    "                )\n",
    "            ),\n",
    "            reraise=True,\n",
    "        )\n",
    "        def openai_call():\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                temperature=0,\n",
    "                messages=prompt,\n",
    "            )\n",
    "            return response\n",
    "\n",
    "        llm_output = \"\"\n",
    "        client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        try:\n",
    "            response = openai_call()\n",
    "            llm_output = response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            logger.error(\n",
    "                \"\"\"Exception %s still occurred after retries on OpenAI API.\n",
    "                         Skipping document %s...\"\"\",\n",
    "                e,\n",
    "                prompt[-1][\"content\"][5:100],\n",
    "            )\n",
    "\n",
    "        return llm_output\n",
    "    \n",
    "llm_model = GPT4oMiniGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(spacy_model, corpus=spacy_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_cterm_extraction(context: str) -> list[dict[str, str]]:\n",
    "    \"\"\"Prompt template for concept term extraction with ChatCompletion OpenAI model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    context: str\n",
    "        The context to add in the prompt template.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[dict[str, str]]\n",
    "        ChatCompletion prompt template.\n",
    "    \"\"\"\n",
    "    prompt_template = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an helpful assistant helping building an ontology.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Extract the most meaningful keywords of the following text. Keep only keywords that could be concepts and not relations. Write them as a list of string with double quotes.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": 'Here is an example. Text: This python package is about ontology learning. I do not know a lot about this field.\\n[\"python package\", \"ontology learning\", \"field\"]',\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Text: {context}\"},\n",
    "    ]\n",
    "    return prompt_template\n",
    "llm_cterm_extraction = LLMTermExtraction(prompt_cterm_extraction, llm_model)\n",
    "llm_cterm_extraction.run(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_concept_extraction = AgglomerativeClusteringConceptExtraction(distance_threshold=0.2, embedding_model=\"dangvantuan/sentence-camembert-large\")\n",
    "ac_concept_extraction.run(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_rterm_extraction(context: str) -> list[dict[str, str]]:\n",
    "    \"\"\"Prompt template for relation term extraction with ChatCompletion OpenAI model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    context: str\n",
    "        The context to add in the prompt template.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[Dict[str, str]]\n",
    "        ChatCompletion prompt template.\n",
    "    \"\"\"\n",
    "    prompt_template = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an helpful assistant helping building an ontology.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Extract the most meaningful words describing actions or states in the following text. Keep only words that could be relations and not concepts. Write them as a list of string with double quotes.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": 'Here is an example. Text: I plan to eat pizza tonight. I am looking for advice.\\n[\"plan\", \"eat\", \"looking for\"]',\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Text: {context}\"},\n",
    "    ]\n",
    "    return prompt_template\n",
    "\n",
    "llm_rterm_extraction = LLMTermExtraction(prompt_rterm_extraction, llm_model)\n",
    "llm_rterm_extraction.run(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_relation_extraction = AgglomerativeClusteringRelationExtraction(distance_threshold=0.2, embedding_model=\"dangvantuan/sentence-camembert-large\", concept_max_distance=8, scope=\"sent\")\n",
    "ac_relation_extraction.run(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_hierarchisation(\n",
    "    doc_context: str, concepts_description: str\n",
    ") -> list[dict[str, str]]:\n",
    "    \"\"\"Prompt template for hierarchisation with ChatCompletion OpenAI model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_context: str\n",
    "        Extract of document contents where concepts appear to use as context.\n",
    "    concepts_description: str\n",
    "        Textual description of the concepts.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[Dict[str, str]]\n",
    "        ChatCompletion prompt template.\n",
    "    \"\"\"\n",
    "    prompt_template = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an helpful assistant helping building an ontology.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"Based on the context given, define if there is a relevant hierarchy between the listed concepts.\n",
    "            The result should be given as a list of list of string with double quotes without any other content.\"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"Here is an example. Concepts: animal, mammal, dog(canine), flower\n",
    "            [[\"mammal\",\"is_generalised_by\",\"animal\"], [\"dog\",\"is_generalised_by\",\"mammal\"], [\"dog\",\"is_generalised_by\",\"animal\"]]\"\"\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Context: {doc_context}\"},\n",
    "        {\"role\": \"user\", \"content\": concepts_description},\n",
    "    ]\n",
    "    return prompt_template\n",
    "\n",
    "llm_concept_hierarchy = LLMBasedHierarchisation(prompt_hierarchisation, llm_model, 20000)\n",
    "llm_concept_hierarchy.run(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kr_serialiser = KRJSONSerialiser()\n",
    "kr_serialiser.serialise(kr=pipeline.kr, file_path=KR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axiom_generators = {    \n",
    "    kr_concepts_to_owl_classes,\n",
    "    kr_relations_to_owl_obj_props,\n",
    "    kr_metarelations_to_owl,\n",
    "    kr_relations_to_anonymous_some_parent,\n",
    "    concept_lrs_to_owl_individuals\n",
    "}\n",
    "owl_axiom_extraction = OWLAxiomExtraction(\n",
    "    owl_axiom_generators=axiom_generators,\n",
    "    base_uri=\"https://github.com/schmarion/phd-code/o/example#\"\n",
    ")\n",
    "\n",
    "owl_axiom_extraction.run(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.kr.rdf_graph.serialize(ONTO_OWL_PATH, format=\"ttl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
